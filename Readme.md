# HpT: Hybrid Acceleration of Spatio-Temporal Attention Model Training on Heterogeneous Manycore Architecture

A hardware- software-based codesign that implements lightweight training for self-attention mechanisms on various spatiotemporal datasets and
accelerates it using heterogeneous architecture. Our parameter-efficient training approach incorporates intrinsic dimensionality
to reduce the trainable parameters, achieving efficiency in training without significant loss in accuracy. 

## Citation
Paper under review

Dahal, S., Dhingra, P., Thapa, KK., Pande, P. & Kalyanaraman, A. HpT: Hybrid Acceleration of Spatio-Temporal Attention Model Training on Heterogeneous Manycore Architecture (2024)


